{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "from ethos.constants import PROJECT_ROOT\n",
    "\n",
    "data_dir = PROJECT_ROOT / \"data/tokenized_datasets\"\n",
    "\n",
    "dataset_dir = data_dir / \"mimic_synth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_orig = pl.read_csv(dataset_dir / \"big/code_counts.csv\")\n",
    "\n",
    "group_expr = (\n",
    "    pl.when(\n",
    "        pl.col(\"code\").str.starts_with(\"ATC\")\n",
    "        & ~pl.col(\"code\").str.starts_with(\"ATC//4//\")\n",
    "        & ~pl.col(\"code\").str.starts_with(\"ATC//SFX//\")\n",
    "    )\n",
    "    .then(pl.lit(\"ATC\"))\n",
    "    .when(pl.col(\"code\").str.slice(0, 3).is_in([\"ICD\", \"ATC\"]))\n",
    "    .then(\n",
    "        pl.col(\"code\").str.slice(0, 3)\n",
    "        + pl.lit(\"_\")\n",
    "        + pl.col(\"code\").str.split(\"//\").list.get(1, null_on_oob=True)\n",
    "    )\n",
    "    .otherwise(pl.col(\"code\").str.split(\"//\").list.get(0))\n",
    "    .alias(\"code\")\n",
    ")\n",
    "\n",
    "df = (\n",
    "    counts_orig.group_by(group_expr)\n",
    "    .agg(pl.sum(\"count\"), pl.count(\"count\").alias(\"n\"))\n",
    "    .sort(\"count\", descending=True, nulls_last=True)\n",
    "    # .filter((pl.col(\"count\") != pl.col(\"count_2\")) | (pl.col(\"n\") != pl.col(\"n_2\")))\n",
    "    # .filter(pl.col( \"code\" ).str.to_uppercase().str.contains(\"BLOOD\"))\n",
    ")\n",
    "\n",
    "sfx_to_counts = [\n",
    "    (sfx, pl.read_csv(dataset_dir / f\"big{sfx}/code_counts.csv\"))\n",
    "    for sfx in (\n",
    "        \"_synth\",\n",
    "        \"_synth_temp0.9\",\n",
    "        \"_synth_temp0.7\",\n",
    "        \"_synth_temp1.1\",\n",
    "    )\n",
    "]\n",
    "\n",
    "for sfx, counts in sfx_to_counts:\n",
    "    df = df.join(\n",
    "        (counts.group_by(group_expr).agg(pl.sum(\"count\"), pl.count(\"count\").alias(\"n\"))),\n",
    "        on=\"code\",\n",
    "        how=\"full\",\n",
    "        suffix=f\"_{sfx}\",\n",
    "        coalesce=True,\n",
    "    )\n",
    "df = df.sort(\"count\", descending=True, nulls_last=True).rename(\n",
    "    {\n",
    "        \"code\": \"Code Group\",\n",
    "        \"count\": \"count__original\",\n",
    "        \"n\": \"n__original\",\n",
    "        \"count__synth\": \"count__synth_temp1\",\n",
    "        \"n__synth\": \"n__synth_temp1\",\n",
    "    }\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = df.sum()\n",
    "total[0, \"Code Group\"] = \"Total\"\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pdf = pl.concat([df, df.sum()]).with_columns(\n",
    "    pl.selectors.numeric().map_elements(lambda v: f\"{v:,}\", return_dtype=pl.Utf8)\n",
    ")\n",
    "pdf = pdf.with_columns(pl.col(\"Code Group\").fill_null(\"Total\")).to_pandas()\n",
    "pdf.columns = pd.MultiIndex.from_tuples(col.split(\"__\")[::-1] for col in pdf.columns)\n",
    "print(\n",
    "    pdf.to_latex(\n",
    "        index=False,\n",
    "        multicolumn=True,\n",
    "        multicolumn_format=\"c\",\n",
    "        escape=True,\n",
    "        column_format=\"l\" + \"c\" * (len(pdf.columns) - 1),\n",
    "        label=\"tab:token-summary-in-synthetic\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from ethos.datasets import TimelineDataset\n",
    "\n",
    "data_stats = []\n",
    "\n",
    "for fold_dir in dataset_dir.glob(\"train*\"):\n",
    "    d = TimelineDataset(fold_dir)\n",
    "    timeline_lengths = [\n",
    "        length\n",
    "        for shard in d._data.shards\n",
    "        for length in shard[\"patient_offsets\"][1:] - shard[\"patient_offsets\"][:-1]\n",
    "    ]\n",
    "\n",
    "    quantiles = np.quantile(timeline_lengths, [0, 0.25, 0.5, 0.75, 1]).astype(int).tolist()\n",
    "    data_stats.append((fold_dir.name, f\"{len(d.tokens):,}\", quantiles))\n",
    "\n",
    "q_cols = [\"min\", \"25%\", \"50%\", \"75%\", \"max\"]\n",
    "data_stats = (\n",
    "    pl.DataFrame(\n",
    "        data_stats,\n",
    "        schema=[\"dataset\", \"num_tokens\", \"timeline_lengths\"],\n",
    "        orient=\"row\",\n",
    "    )\n",
    "    .with_columns(pl.col(\"timeline_lengths\").list.to_struct(fields=q_cols).struct.unnest())\n",
    "    .with_columns(pl.col(q_cols).map_elements(lambda s: f\"{s:,}\", return_dtype=pl.String))\n",
    "    .drop(\"timeline_lengths\")\n",
    ")\n",
    "data_stats"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ethos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
