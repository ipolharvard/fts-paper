{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "from ethos.constants import PROJECT_ROOT\n",
    "from ethos.datasets import ReadmissionDataset, TimelineDataset\n",
    "from ethos.metrics.fidelity import (\n",
    "    convert_numpy,\n",
    "    fetch_all_codes,\n",
    "    fidelity_evaluation,\n",
    "    fidelity_from_halo_evaluation,\n",
    "    transform_data_matrix,\n",
    ")\n",
    "\n",
    "data_dir = PROJECT_ROOT / \"data/tokenized_datasets\"\n",
    "\n",
    "# this notebook requires tokenized datasets, that we cannot share due to the MIMIC sharing policy,\n",
    "# however everything can be recreated using this repository and the result files on GDrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_data(real_key, synth_key, data_dir, dataset_type=\"readmission\"):\n",
    "    \"\"\"Loads and prepares data using the specified dataset type.\"\"\"\n",
    "\n",
    "    if dataset_type == \"readmission\":\n",
    "        DatasetClass = ReadmissionDataset\n",
    "    elif dataset_type == \"timeline\":\n",
    "        DatasetClass = TimelineDataset\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported dataset_type: {dataset_type}\")\n",
    "\n",
    "    real_data = DatasetClass(data_dir / f\"mimic_synth/{real_key}\")\n",
    "    synth_data = DatasetClass(data_dir / f\"mimic_synth/{synth_key}\")\n",
    "\n",
    "    print(\"number of patient_ids in synth_data:\", len(set(synth_data.patient_ids.tolist())))\n",
    "    print(\"number of static_data in synth_data:\", len(set(synth_data.static_data.keys())))\n",
    "\n",
    "    if set(synth_data.patient_ids.tolist()) == set(synth_data.static_data.keys()):\n",
    "        print(f\"[{real_key} vs {synth_key}] Patient IDs match\")\n",
    "        if real_data.vocab.stoi == synth_data.vocab.stoi:\n",
    "            print(f\"[{real_key} vs {synth_key}] Vocabularies are the same\")\n",
    "            code_vocab_size = len(real_data.vocab.stoi)\n",
    "        else:\n",
    "            print(f\"[{real_key} vs {synth_key}] Vocabularies are different\")\n",
    "            code_vocab_size = len(real_data.vocab.stoi)\n",
    "\n",
    "        real_data_all_codes = fetch_all_codes(real_data, dataset_type)\n",
    "        synthetic_data_all_codes = fetch_all_codes(synth_data, dataset_type)\n",
    "\n",
    "        def prepare_matrices(all_codes):\n",
    "            return {\n",
    "                mt: transform_data_matrix(all_codes, code_vocab_size, mt, dataset_type=dataset_type)\n",
    "                for mt in [\"binary\", \"count\", \"probability\"]\n",
    "            }\n",
    "\n",
    "        return (\n",
    "            real_data,\n",
    "            synth_data,\n",
    "            prepare_matrices(real_data_all_codes),\n",
    "            prepare_matrices(synthetic_data_all_codes),\n",
    "            real_data_all_codes,\n",
    "            synthetic_data_all_codes,\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"[{real_key} vs {synth_key}] Patient IDs do not match\")\n",
    "\n",
    "\n",
    "def evaluate_pair(real_key, synth_key, data_dir, save_dir, dataset_type=\"readmission\"):\n",
    "    \"\"\"Evaluate a real vs synthetic dataset pair.\"\"\"\n",
    "\n",
    "    _, _, real_matrices, synth_matrices, real_codes, synthetic_codes = load_and_prepare_data(\n",
    "        real_key, synth_key, data_dir, dataset_type=dataset_type\n",
    "    )\n",
    "\n",
    "    pair_results = defaultdict(lambda: defaultdict(dict))\n",
    "    pair_results[\"fidelity_from_halo\"] = fidelity_from_halo_evaluation(\n",
    "        real_codes, synthetic_codes, save_dir, dataset_type=dataset_type\n",
    "    )\n",
    "\n",
    "    for mt in [\"binary\", \"count\", \"probability\"]:\n",
    "        print(f\"Evaluating {real_key} vs {synth_key} with {mt} matrix...\")\n",
    "        fidelity_res = fidelity_evaluation(real_matrices[mt], synth_matrices[mt], mt)\n",
    "        pair_results[\"fidelity\"].update(fidelity_res)\n",
    "\n",
    "    return pair_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = PROJECT_ROOT / \"results/fidelity\"\n",
    "dataset_type_to_save_dir = {\n",
    "    \"readmission\": result_dir / \"readmission\",\n",
    "    \"timeline\": result_dir / \"timeline\",\n",
    "}\n",
    "\n",
    "real_types = [\"big\", \"small\", \"little\"]\n",
    "synth_suffixes = [\"_synth\", \"_synth_temp0.7\", \"_synth_temp0.9\", \"_synth_temp1.1\"]\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "for dataset_type, base_save_dir in dataset_type_to_save_dir.items():\n",
    "    print(f\"\\n========== Starting evaluations for dataset_type: {dataset_type} ==========\")\n",
    "    for real_key in real_types:\n",
    "        for suffix in synth_suffixes:\n",
    "            synth_key = real_key + suffix\n",
    "            print(f\"\\nStarting evaluation for {real_key} vs {synth_key}\")\n",
    "            result_key = f\"{dataset_type}_{real_key}_vs_{synth_key}\"\n",
    "\n",
    "            current_save_dir = base_save_dir / synth_key\n",
    "            current_save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            try:\n",
    "                all_results[result_key] = evaluate_pair(\n",
    "                    real_key, synth_key, data_dir, current_save_dir, dataset_type=dataset_type\n",
    "                )\n",
    "\n",
    "                with (current_save_dir / f\"{result_key}_results.json\").open(\"w\") as f:\n",
    "                    json.dump(all_results[result_key], f, indent=4, default=convert_numpy)\n",
    "\n",
    "                print(f\"Finished evaluation for {result_key}\")\n",
    "                print(f\"Results for {result_key}: {all_results[result_key]}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error occurred during evaluation for {result_key}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
