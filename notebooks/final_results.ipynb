{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import polars as pl\n",
    "\n",
    "from ethos.constants import PROJECT_ROOT\n",
    "from ethos.inference.constants import Task\n",
    "from ethos.metrics import compute_drg_results, compute_metrics, compute_sofa_results\n",
    "from ethos.task_processing import TASK_RESULTS_PROCESSING_FUNC, join_metadata\n",
    "\n",
    "color = \"#00A1D9\"\n",
    "\n",
    "\n",
    "def our_join_metadata(df: pl.DataFrame, input_dir: Path) -> pl.DataFrame:\n",
    "    return join_metadata(df, input_dir).select(\n",
    "        pl.col(\"model_fp\").str.split(\"/\").list[-4].alias(\"dataset\"),\n",
    "        pl.col(\"model_fp\").str.split(\"/\").list[-2].alias(\"model\").str.slice(len(\"layer_3_do_0.3_\")),\n",
    "        pl.col(\"temperature\").alias(\"temp\"),\n",
    "        *df.columns,\n",
    "    )\n",
    "\n",
    "\n",
    "def is_valid_file(fp: Path) -> bool:\n",
    "    folds = [\"little\", \"small\", \"big\"]\n",
    "    folds.extend([f + \"_synth\" for f in folds])\n",
    "    return (\n",
    "        fp.name.startswith(\"mimic_synth\")\n",
    "        and any(s in fp.name for s in (\"+\", *[fold + \"_best\" for fold in folds]))\n",
    "        and \"val\" not in fp.name\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_forest(\n",
    "    df: pl.DataFrame,\n",
    "    x: str = \"fitted_auc\",\n",
    "    x_ci: str = \"fitted_auc_ci\",\n",
    "    y: str = \"model\",\n",
    "    title: str = \"\",\n",
    "    lw=3,\n",
    "    color=color,\n",
    "    sort_expr: str | pl.Expr = None,\n",
    "):\n",
    "    if sort_expr is None:\n",
    "        sort_expr = x\n",
    "\n",
    "    df = df.sort(sort_expr, nulls_last=True)\n",
    "    for i, (m, (lo, hi)) in enumerate(df[x, x_ci].rows()):\n",
    "        plt.plot([lo, hi], [i, i], color=color, lw=lw)\n",
    "        plt.plot([lo, lo], [i - 0.3, i + 0.3], color=color, lw=lw)\n",
    "        plt.plot([hi, hi], [i - 0.3, i + 0.3], color=color, lw=lw)\n",
    "        plt.plot(m, i, marker=\"D\", color=color, markersize=10)\n",
    "\n",
    "    plt.yticks(list(range(len(df))), df[y])\n",
    "    plt.grid(True)\n",
    "    plt.title(title)\n",
    "\n",
    "\n",
    "n_bootstraps = 1000\n",
    "\n",
    "dataset_name = \"mimic_synth\"\n",
    "result_dir = PROJECT_ROOT / \"results\"\n",
    "all_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "drg_results_dir = result_dir / Task.DRG_PREDICTION\n",
    "drg_process_func = TASK_RESULTS_PROCESSING_FUNC[Task.DRG_PREDICTION]\n",
    "all_results[Task.DRG_PREDICTION] = (\n",
    "    pl.concat(\n",
    "        drg_process_func(fp, top_k=1)\n",
    "        .pipe(compute_drg_results, n_bootstraps=n_bootstraps)\n",
    "        .with_columns(name=pl.lit(fp.name))\n",
    "        for fp in drg_results_dir.iterdir()\n",
    "        if is_valid_file(fp)\n",
    "    )\n",
    "    .pipe(our_join_metadata, drg_results_dir)\n",
    "    .drop(\"name\")\n",
    "    .sort(\"acc_top_1\", descending=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_forest(\n",
    "    all_results[Task.DRG_PREDICTION],\n",
    "    x=\"acc_top_1\",\n",
    "    x_ci=\"acc_top_1_ci\",\n",
    "    y=\"model\",\n",
    "    title=\"DRG Prediction\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sofa_results_dir = result_dir / Task.SOFA_PREDICTION\n",
    "sofa_process_func = TASK_RESULTS_PROCESSING_FUNC[Task.SOFA_PREDICTION]\n",
    "all_results[Task.SOFA_PREDICTION] = (\n",
    "    pl.concat(\n",
    "        pl.from_dict(\n",
    "            sofa_process_func(fp).pipe(\n",
    "                lambda tdf: compute_sofa_results(\n",
    "                    *tdf[\"true_sofa\", \"pred_sofa\"], n_bootstraps=n_bootstraps\n",
    "                )\n",
    "            )\n",
    "        ).with_columns(name=pl.lit(fp.name))\n",
    "        for fp in sofa_results_dir.iterdir()\n",
    "        if is_valid_file(fp)\n",
    "    )\n",
    "    .with_columns(\n",
    "        r2=pl.col(\"r2\").struct[\"score\"],\n",
    "        r2_ci=pl.concat_list(pl.col(\"r2\").struct[\"ci_low\"], pl.col(\"r2\").struct[\"ci_high\"]),\n",
    "        mae=pl.col(\"mae\").struct[\"score\"],\n",
    "        mae_ci=pl.concat_list(pl.col(\"mae\").struct[\"ci_low\"], pl.col(\"mae\").struct[\"ci_high\"]),\n",
    "    )\n",
    "    .pipe(our_join_metadata, sofa_results_dir)\n",
    "    .drop(\"name\")\n",
    "    .sort(\"r2\", descending=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_forest(\n",
    "    all_results[Task.SOFA_PREDICTION],\n",
    "    x=\"r2\",\n",
    "    x_ci=\"r2_ci\",\n",
    "    y=\"model\",\n",
    "    title=\"SOFA Prediction\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_results(task: Task, n_bootstraps: int, **kwargs) -> pl.DataFrame:\n",
    "    proc_func = TASK_RESULTS_PROCESSING_FUNC[task]\n",
    "\n",
    "    def compute_metrics_for_single_case(fp):\n",
    "        df = proc_func(fp, **kwargs)\n",
    "        res = compute_metrics(*df[\"expected\", \"actual\"], n_bootstraps=n_bootstraps)\n",
    "        return {\"name\": fp.name, **res, \"rep_num\": df[\"counts\"].mean()}\n",
    "\n",
    "    task_results_dir = result_dir / task\n",
    "    return (\n",
    "        pl.DataFrame(\n",
    "            compute_metrics_for_single_case(fp)\n",
    "            for fp in task_results_dir.iterdir()\n",
    "            if is_valid_file(fp)\n",
    "        )\n",
    "        .pipe(our_join_metadata, task_results_dir)\n",
    "        .drop(\"name\")\n",
    "        .sort(\"fitted_auc\", descending=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results[Task.READMISSION] = compute_results(Task.READMISSION, n_bootstraps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_forest(all_results[Task.READMISSION], title=\"30-day Hospital Readmission\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results[Task.ICU_ADMISSION] = compute_results(\n",
    "    Task.ICU_ADMISSION, n_bootstraps, warn_on_dropped=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_forest(all_results[Task.ICU_ADMISSION], title=\"ICU Admission\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results[Task.HOSPITAL_MORTALITY] = compute_results(\n",
    "    Task.HOSPITAL_MORTALITY, n_bootstraps, warn_on_dropped=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_forest(all_results[Task.HOSPITAL_MORTALITY], title=\"Hospital Mortality\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ethos.metrics.paper_fts import col_to_title, eval_overall_results, join_results, score_to_str\n",
    "\n",
    "tasks = [\n",
    "    Task.DRG_PREDICTION,\n",
    "    Task.SOFA_PREDICTION,\n",
    "    Task.READMISSION,\n",
    "    Task.ICU_ADMISSION,\n",
    "    Task.HOSPITAL_MORTALITY,\n",
    "]\n",
    "\n",
    "on = \"model\"\n",
    "\n",
    "df = join_results(all_results, on=on).select(\"model\", *tasks)\n",
    "df = df.join(eval_overall_results(df), on=on).sort(\n",
    "    pl.col(\"overall_score\").struct[\"score\"], descending=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ethos.metrics.paper_fts import print_overall_score\n",
    "\n",
    "ax = print_overall_score(df, figsize=(4, 3))\n",
    "ax.set_ylabel(\"Training Dataset\")\n",
    "ax.set_xlabel(\"Overall Score (95% CI)\")\n",
    "\n",
    "figure_dir = PROJECT_ROOT / \"figures\"\n",
    "figure_dir.mkdir(exist_ok=True, parents=True)\n",
    "plt.savefig(\n",
    "    figure_dir / \"4_final_results.pdf\",\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    df.rename(col_to_title, strict=False)\n",
    "    .with_columns(\n",
    "        pl.exclude(pl.Utf8).map_elements(score_to_str, return_dtype=pl.Utf8),\n",
    "    )\n",
    "    .to_pandas()\n",
    "    .to_latex(\n",
    "        index=False,\n",
    "        column_format=\"l\" + \"c\" * (len(df.columns) - 1),\n",
    "        escape=True,\n",
    "        label=\"tab:stage4-final-results\",\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ethos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
