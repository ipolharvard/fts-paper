{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import polars as pl\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from ethos.constants import PROJECT_ROOT\n",
    "from ethos.inference.constants import Task\n",
    "from ethos.metrics import compute_drg_results, compute_metrics, compute_sofa_results\n",
    "from ethos.task_processing import TASK_RESULTS_PROCESSING_FUNC, join_metadata\n",
    "\n",
    "color = \"#00A1D9\"\n",
    "\n",
    "\n",
    "def our_join_metadata(df: pl.DataFrame, input_dir: Path) -> pl.DataFrame:\n",
    "    return join_metadata(df, input_dir).select(\n",
    "        pl.col(\"model_fp\").str.split(\"/\").list[-4].alias(\"dataset\"),\n",
    "        pl.col(\"model_fp\").str.split(\"/\").list[-2].alias(\"model\"),\n",
    "        pl.col(\"input_dir\").str.split(\"/\").list.last().alias(\"fold\"),\n",
    "        pl.col(\"temperature\").alias(\"temp\"),\n",
    "        *df.columns,\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_forest(\n",
    "    ax: plt.Axes,\n",
    "    df: pl.DataFrame,\n",
    "    x: str = \"model\",\n",
    "    y: str = \"fitted_auc\",\n",
    "    y_ci: str = \"fitted_auc_ci\",\n",
    "    title: str = \"\",\n",
    "    lw=3,\n",
    "    color=color,\n",
    "    sort_expr: str | pl.Expr = None,\n",
    "):\n",
    "    if sort_expr is None:\n",
    "        sort_expr = y\n",
    "\n",
    "    df = df.sort(sort_expr, nulls_last=True)\n",
    "    for i, (m, (lo, hi)) in enumerate(df[y, y_ci].rows()):\n",
    "        ax.plot([i, i], [lo, hi], color=color, lw=lw)\n",
    "        ax.plot([i - 0.3, i + 0.3], [lo, lo], color=color, lw=lw)\n",
    "        ax.plot([i - 0.3, i + 0.3], [hi, hi], color=color, lw=lw)\n",
    "        ax.plot(i, m, marker=\"D\", color=color, markersize=10)\n",
    "\n",
    "    ax.set_xticks(list(range(len(df))))\n",
    "    ax.set_xticklabels(df[x])\n",
    "    ax.grid(True)\n",
    "    ax.set_title(title)\n",
    "\n",
    "\n",
    "def plot_two_forests(data: pl.DataFrame, **kwargs):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "    title = kwargs.pop(\"title\", \"\")\n",
    "    data1 = data.filter(fold=\"val1\").with_columns(\n",
    "        model=(\n",
    "            pl.col(\"model\").str.split(\"_\").list.last().cast(pl.Int64, strict=False).cast(pl.Utf8)\n",
    "            + pl.lit(\"/20\")\n",
    "        ).fill_null(\"full\")\n",
    "    )\n",
    "    plot_forest(\n",
    "        axes[0],\n",
    "        data1,\n",
    "        x=\"model\",\n",
    "        title=f\"{title} varying size\",\n",
    "        sort_expr=pl.col(\"model\").str.split(\"/\").list.first().cast(pl.Int64, strict=False),\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "    data2 = data.filter(fold=\"val2\")\n",
    "    plot_forest(\n",
    "        axes[1], data2, x=\"temp\", title=f\"{title} varying temperatures\", sort_expr=\"temp\", **kwargs\n",
    "    )\n",
    "\n",
    "\n",
    "n_bootstraps = 1000\n",
    "\n",
    "\n",
    "def is_file_valid(fp: Path) -> bool:\n",
    "    return fp.name.startswith(dataset_name) and \"val\" in fp.name\n",
    "\n",
    "\n",
    "dataset_name = \"mimic_synth\"\n",
    "result_dir = PROJECT_ROOT / \"results\"\n",
    "all_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "drg_results_dir = result_dir / Task.DRG_PREDICTION\n",
    "drg_process_func = TASK_RESULTS_PROCESSING_FUNC[Task.DRG_PREDICTION]\n",
    "all_results[Task.DRG_PREDICTION] = (\n",
    "    pl.concat(\n",
    "        drg_process_func(fp, top_k=1)\n",
    "        .pipe(compute_drg_results, n_bootstraps=n_bootstraps)\n",
    "        .with_columns(name=pl.lit(fp.name))\n",
    "        for fp in drg_results_dir.iterdir()\n",
    "        if is_file_valid(fp)\n",
    "    )\n",
    "    .pipe(our_join_metadata, drg_results_dir)\n",
    "    .sort(\"acc_top_1\", descending=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_two_forests(all_results[Task.DRG_PREDICTION], y=\"acc_top_1\", y_ci=\"acc_top_1_ci\", title=\"DRG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sofa_results_dir = result_dir / Task.SOFA_PREDICTION\n",
    "sofa_process_func = TASK_RESULTS_PROCESSING_FUNC[Task.SOFA_PREDICTION]\n",
    "all_results[Task.SOFA_PREDICTION] = (\n",
    "    pl.concat(\n",
    "        pl.from_dict(\n",
    "            sofa_process_func(fp).pipe(\n",
    "                lambda tdf: compute_sofa_results(\n",
    "                    *tdf[\"true_sofa\", \"pred_sofa\"], n_bootstraps=n_bootstraps\n",
    "                )\n",
    "            )\n",
    "        ).with_columns(name=pl.lit(fp.name))\n",
    "        for fp in sofa_results_dir.iterdir()\n",
    "        if is_file_valid(fp)\n",
    "    )\n",
    "    .with_columns(\n",
    "        r2=pl.col(\"r2\").struct[\"score\"],\n",
    "        r2_ci=pl.concat_list(pl.col(\"r2\").struct[\"ci_low\"], pl.col(\"r2\").struct[\"ci_high\"]),\n",
    "        mae=pl.col(\"mae\").struct[\"score\"],\n",
    "        mae_ci=pl.concat_list(pl.col(\"mae\").struct[\"ci_low\"], pl.col(\"mae\").struct[\"ci_high\"]),\n",
    "    )\n",
    "    .pipe(our_join_metadata, sofa_results_dir)\n",
    "    .sort(\"r2\", descending=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_two_forests(all_results[Task.SOFA_PREDICTION], y=\"r2\", y_ci=\"r2_ci\", title=\"SOFA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_results(task: Task, n_bootstraps: int, **kwargs) -> pl.DataFrame:\n",
    "    proc_func = TASK_RESULTS_PROCESSING_FUNC[task]\n",
    "\n",
    "    def compute_metrics_for_single_case(fp):\n",
    "        df = proc_func(fp, **kwargs)\n",
    "        res = compute_metrics(*df[\"expected\", \"actual\"], n_bootstraps=n_bootstraps)\n",
    "        return {\"name\": fp.name, **res, \"rep_num\": df[\"counts\"].mean()}\n",
    "\n",
    "    task_results_dir = result_dir / task\n",
    "    return (\n",
    "        pl.DataFrame(\n",
    "            compute_metrics_for_single_case(fp)\n",
    "            for fp in task_results_dir.iterdir()\n",
    "            if is_file_valid(fp)\n",
    "        )\n",
    "        .pipe(our_join_metadata, task_results_dir)\n",
    "        .drop(\"name\")\n",
    "        .sort(\"fitted_auc\", descending=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results[Task.READMISSION] = compute_results(\n",
    "    Task.READMISSION, n_bootstraps, warn_on_dropped=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_two_forests(all_results[Task.READMISSION], title=\"30-day Readmission\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results[Task.ICU_ADMISSION] = compute_results(\n",
    "    Task.ICU_ADMISSION, n_bootstraps, warn_on_dropped=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_two_forests(all_results[Task.ICU_ADMISSION], title=\"ICU Admission\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results[Task.HOSPITAL_MORTALITY] = compute_results(\n",
    "    Task.HOSPITAL_MORTALITY, n_bootstraps, warn_on_dropped=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_two_forests(all_results[Task.HOSPITAL_MORTALITY], title=\"Hospital Mortality\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ethos.metrics.paper_fts import col_to_title, eval_overall_results, join_results, score_to_str\n",
    "\n",
    "df = (\n",
    "    join_results({task: df.filter(fold=\"val1\") for task, df in all_results.items()}, on=\"model\")\n",
    "    .with_columns(\n",
    "        (\n",
    "            pl.col(\"model\").str.split(\"_\").list.last().cast(pl.Int64, strict=False).fill_null(20)\n",
    "            / 20\n",
    "        ).map_elements(lambda x: f\"{x:.0%}\", return_dtype=pl.Utf8)\n",
    "    )\n",
    "    .reverse()\n",
    ")\n",
    "\n",
    "df = df.join(eval_overall_results(df), on=\"model\", maintain_order=\"left\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ethos.metrics.paper_fts import print_overall_score\n",
    "\n",
    "ax = print_overall_score(df, figsize=(4, 2))\n",
    "ax.set_ylabel(\"Fraction of Data\")\n",
    "ax.set_xlabel(\"Overall Score (95% CI)\")\n",
    "\n",
    "figure_dir = PROJECT_ROOT / \"figures\"\n",
    "figure_dir.mkdir(exist_ok=True, parents=True)\n",
    "plt.savefig(\n",
    "    figure_dir / \"1_train_division.pdf\",\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    df.rename(col_to_title, strict=False)\n",
    "    .with_columns(pl.exclude(pl.Utf8).map_elements(score_to_str, return_dtype=pl.Utf8))\n",
    "    .to_pandas()\n",
    "    .to_latex(\n",
    "        index=False,\n",
    "        column_format=\"l\" + \"c\" * (len(df.columns) - 1),\n",
    "        escape=True,\n",
    "        label=\"tab:stage1-train-division\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = join_results(\n",
    "    {task: df.filter(fold=\"val2\") for task, df in all_results.items()},\n",
    "    on=\"temp\",\n",
    "    sort=pl.col(\"temp\").cast(pl.Float64),\n",
    ")\n",
    "\n",
    "df = df.join(eval_overall_results(df), on=\"temp\", maintain_order=\"left\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = print_overall_score(df, figsize=(4, 2))\n",
    "ax.set_ylabel(\"Inference Temperature\")\n",
    "ax.set_xlabel(\"Overall Score (95% CI)\")\n",
    "plt.savefig(\n",
    "    figure_dir / \"2_inference_temperature.pdf\",\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    df.rename(col_to_title, strict=False)\n",
    "    .with_columns(pl.exclude(pl.Utf8).map_elements(score_to_str, return_dtype=pl.Utf8))\n",
    "    .to_pandas()\n",
    "    .to_latex(\n",
    "        index=False,\n",
    "        column_format=\"l\" + \"c\" * (len(df.columns) - 1),\n",
    "        escape=True,\n",
    "        label=\"tab:stage2-inference-temperature\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib.offsetbox import AnchoredText\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.metrics import brier_score_loss\n",
    "\n",
    "n_bins = 10\n",
    "\n",
    "tasks = [\n",
    "    Task.READMISSION,\n",
    "    Task.ICU_ADMISSION,\n",
    "    Task.HOSPITAL_MORTALITY,\n",
    "]\n",
    "\n",
    "\n",
    "fns = [\n",
    "    (\"mimic_synth_layer_3_do_0.3_full_val2_best_egaf72qa\", 1),\n",
    "    (\"mimic_synth_layer_3_do_0.3_full_val2_best_temp0.9_egaf72qa\", 0.9),\n",
    "    (\"mimic_synth_layer_3_do_0.3_full_val2_best_temp0.8_egaf72qa\", 0.8),\n",
    "]\n",
    "\n",
    "size = 8\n",
    "n_rows, n_cols = len(fns), len(tasks)\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(size, size * n_rows / n_cols))\n",
    "for i, (fn, temperature) in enumerate(fns):\n",
    "    for j, task in enumerate(tasks):\n",
    "        df = TASK_RESULTS_PROCESSING_FUNC[task](result_dir / task / fn, warn_on_dropped=False)\n",
    "\n",
    "        frac_pos, mean_pred = calibration_curve(*df[\"expected\", \"actual\"], n_bins=n_bins)\n",
    "\n",
    "        bootstrapped_fracs = np.zeros((n_bootstraps, len(mean_pred)))\n",
    "        for seed in range(n_bootstraps):\n",
    "            frac_bs, mean_pred_bs = calibration_curve(\n",
    "                *df[\"expected\", \"actual\"].sample(fraction=1, with_replacement=True, seed=seed),\n",
    "                n_bins=n_bins,\n",
    "            )\n",
    "            bootstrapped_fracs[seed] = (\n",
    "                frac_bs\n",
    "                if len(frac_bs) == len(frac_pos)\n",
    "                else np.interp(mean_pred, mean_pred_bs, frac_bs)\n",
    "            )\n",
    "\n",
    "        ax = axes[i, j]\n",
    "        ci_lower, ci_upper = np.percentile(bootstrapped_fracs, [2.5, 97.5], axis=0)\n",
    "        ax.fill_between(\n",
    "            mean_pred,\n",
    "            ci_lower,\n",
    "            ci_upper,\n",
    "            color=\"gray\",\n",
    "            label=\"95% Confidence Interval\",\n",
    "        )\n",
    "\n",
    "        ax.plot([0, 1], [0, 1], linestyle=\"--\", color=\"black\", label=\"Perfect Calibration\")\n",
    "        ax.plot(\n",
    "            mean_pred,\n",
    "            frac_pos,\n",
    "            color=color,\n",
    "            lw=5,\n",
    "            label=\"ETHOS Calibration\",\n",
    "        )\n",
    "        ax.set_xlim([-0.01, 1.01])\n",
    "        ax.set_ylim([-0.01, 1.01])\n",
    "        if i == len(fns) - 1:\n",
    "            ax.set_xlabel(col_to_title[task])\n",
    "        else:\n",
    "            ax.set_xticks([])\n",
    "\n",
    "        if j == 0:\n",
    "            ax.set_ylabel(f\"{temperature:.1f}\")\n",
    "        else:\n",
    "            ax.set_yticks([])\n",
    "        ax.grid(False)\n",
    "        ax.add_artist(\n",
    "            AnchoredText(\n",
    "                f\"Brier score: {brier_score_loss(df['expected'], df['actual']):.3f}\",\n",
    "                loc=\"lower right\",\n",
    "                pad=0,\n",
    "                borderpad=0.1,\n",
    "                frameon=False,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    fig.supxlabel(\"Binary Downstream Tasks\")\n",
    "    fig.supylabel(\"Inference Temperature\")\n",
    "    fig.subplots_adjust(hspace=0.1, wspace=0.1)\n",
    "    fig.savefig(\n",
    "        PROJECT_ROOT / \"figures\" / \"supp_calibration_curves.pdf\",\n",
    "        dpi=300,\n",
    "        bbox_inches=\"tight\",\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ethos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
